{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0e4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full ingestion + embedding pipeline\n",
      "üìÇ JSON Directory: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\n",
      "üíæ Vector Store Directory: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db1\n",
      "Found 10 JSON files in D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Bank and Financial Institution Act 2073.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Banking Offence and Punishment Act 2064.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Constitution of Nepal 2072.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Electronic Commerce Act 2081.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\International Financial Transactions Act 2054.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Criminal Offences Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Income Tax Act 2058.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Labour Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The National Civil (Code) Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The National Penal (Code) Act 2074.json\n",
      "Normalized and extracted 1930 textual units.\n",
      "\n",
      "=== Building Chroma store for MiniLM (sentence-transformers/all-MiniLM-L6-v2) ===\n",
      "Loading embedding model (this may download weights on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_23132\\3295287062.py:199: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=model_name)\n",
      "d:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-1\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully persisted MiniLM store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db1\\minilm_store\n",
      "   Total documents: 1930\n",
      "\n",
      "=== Building Chroma store for BGE-Base (BAAI/bge-base-en-v1.5) ===\n",
      "Loading embedding model (this may download weights on first run)...\n",
      "‚úÖ Successfully persisted BGE-Base store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db1\\bge_base_store\n",
      "   Total documents: 1930\n",
      "\n",
      "=== Building Chroma store for BGE-Large (BAAI/bge-large-en-v1.5) ===\n",
      "Loading embedding model (this may download weights on first run)...\n",
      "‚úÖ Successfully persisted BGE-Large store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db1\\bge_large_store\n",
      "   Total documents: 1930\n",
      "\n",
      "üîö Pipeline finished. Summary:\n",
      " - MiniLM: ‚úÖ OK\n",
      " - BGE-Base: ‚úÖ OK\n",
      " - BGE-Large: ‚úÖ OK\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain / Chroma imports\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent  # parent of notebooks/\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Import project paths\n",
    "from config.paths import DATA_DIR, VECTORSTORE1_DIR , VECTORSTORE2_DIR\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "JSON_DIR = DATA_DIR / \"processed\"\n",
    "VECTORSTORE_DIR = Path(VECTORSTORE1_DIR)\n",
    "VECTORSTORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBED_MODELS = {\n",
    "    \"MiniLM\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"BGE-Base\": \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BGE-Large\": \"BAAI/bge-large-en-v1.5\",\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def list_json_files(json_dir: Path) -> List[Path]:\n",
    "    files = sorted(list(json_dir.glob(\"*.json\")))\n",
    "    print(f\"Found {len(files)} JSON files in {json_dir}\")\n",
    "    for f in files:\n",
    "        print(\" -\", f)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_json_file(path: Path) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_all_jsons(json_dir: Path) -> List[Any]:\n",
    "    files = list_json_files(json_dir)\n",
    "    all_docs = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            data = load_json_file(p)\n",
    "            all_docs.append({\"__source_file\": p.name, \"__raw\": data})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read {p.name}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "def normalize_and_extract(raw_documents: List[Dict[str, Any]]) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Extract texts and metadata from the new JSON schema:\n",
    "    {\n",
    "      \"document_title\": str,\n",
    "      \"preamble\": str,\n",
    "      \"parts\": [\n",
    "        {\n",
    "          \"part_number\": str,\n",
    "          \"part_title\": str,\n",
    "          \"articles\": [\n",
    "            {\n",
    "              \"article_number\": str,\n",
    "              \"article_title\": str,\n",
    "              \"clauses\": [str]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"schedules\": [\n",
    "        {\n",
    "          \"schedule_number\": str,\n",
    "          \"schedule_title\": str\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    texts: List[str] = []\n",
    "    metadatas: List[Dict[str, Any]] = []\n",
    "\n",
    "    for raw_doc in raw_documents:\n",
    "        source_name = raw_doc.get(\"__source_file\", \"unknown\")\n",
    "        data = raw_doc.get(\"__raw\")\n",
    "\n",
    "        if data is None or not isinstance(data, dict):\n",
    "            continue\n",
    "\n",
    "        document_title = data.get(\"document_title\", \"\")\n",
    "        preamble = data.get(\"preamble\", \"\")\n",
    "        \n",
    "        # Process preamble if exists\n",
    "        if preamble and preamble.strip():\n",
    "            metadata = {\n",
    "                \"source\": source_name,\n",
    "                \"document_title\": document_title,\n",
    "                \"section_type\": \"preamble\",\n",
    "                \"chapter\": \"Preamble\",\n",
    "                \"section\": \"Preamble\"\n",
    "            }\n",
    "            texts.append(preamble.strip())\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        # Process parts and articles\n",
    "        parts = data.get(\"parts\", [])\n",
    "        if not isinstance(parts, list):\n",
    "            continue\n",
    "\n",
    "        for part in parts:\n",
    "            if not isinstance(part, dict):\n",
    "                continue\n",
    "                \n",
    "            part_num = part.get(\"part_number\", \"\")\n",
    "            part_title = part.get(\"part_title\", \"\")\n",
    "            chapter_label = f\"Part {part_num}: {part_title}\".strip()\n",
    "\n",
    "            articles = part.get(\"articles\", [])\n",
    "            if not isinstance(articles, list):\n",
    "                continue\n",
    "\n",
    "            for article in articles:\n",
    "                if not isinstance(article, dict):\n",
    "                    continue\n",
    "                    \n",
    "                art_num = article.get(\"article_number\", \"\")\n",
    "                art_title = article.get(\"article_title\", \"\")\n",
    "                clauses = article.get(\"clauses\", [])\n",
    "                \n",
    "                if not isinstance(clauses, list):\n",
    "                    continue\n",
    "\n",
    "                # Join all clauses into a single text block for this article\n",
    "                combined_text = \"\\n\\n\".join([clause.strip() for clause in clauses if clause and clause.strip()])\n",
    "                \n",
    "                if not combined_text:\n",
    "                    continue\n",
    "\n",
    "                section_label = f\"Article {art_num}: {art_title}\".strip()\n",
    "                metadata = {\n",
    "                    \"source\": source_name,\n",
    "                    \"document_title\": document_title,\n",
    "                    \"section_type\": \"article\",\n",
    "                    \"chapter\": chapter_label,\n",
    "                    \"section\": section_label,\n",
    "                    \"part_number\": part_num,\n",
    "                    \"article_number\": art_num,\n",
    "                    \"num_clauses\": len(clauses)\n",
    "                }\n",
    "                texts.append(combined_text)\n",
    "                metadatas.append(metadata)\n",
    "\n",
    "        # Process schedules\n",
    "        schedules = data.get(\"schedules\", [])\n",
    "        if isinstance(schedules, list):\n",
    "            for schedule in schedules:\n",
    "                if not isinstance(schedule, dict):\n",
    "                    continue\n",
    "                    \n",
    "                schedule_num = schedule.get(\"schedule_number\", \"\")\n",
    "                schedule_title = schedule.get(\"schedule_title\", \"\")\n",
    "                \n",
    "                # Some schedules might have content in the title\n",
    "                if schedule_title and schedule_title.strip():\n",
    "                    section_label = f\"Schedule {schedule_num}\".strip()\n",
    "                    metadata = {\n",
    "                        \"source\": source_name,\n",
    "                        \"document_title\": document_title,\n",
    "                        \"section_type\": \"schedule\",\n",
    "                        \"chapter\": \"Schedules\",\n",
    "                        \"section\": section_label,\n",
    "                        \"schedule_number\": schedule_num\n",
    "                    }\n",
    "                    texts.append(schedule_title.strip())\n",
    "                    metadatas.append(metadata)\n",
    "\n",
    "    print(f\"Normalized and extracted {len(texts)} textual units.\")\n",
    "    return texts, metadatas\n",
    "\n",
    "\n",
    "def build_and_persist_chroma(label: str, model_name: str, texts: List[str], metadatas: List[Dict[str, Any]]):\n",
    "    print(f\"\\n=== Building Chroma store for {label} ({model_name}) ===\")\n",
    "    if not texts:\n",
    "        print(f\"‚ö†Ô∏è No texts to embed for {label}. Skipping.\")\n",
    "        return False\n",
    "\n",
    "    persist_dir = VECTORSTORE_DIR / f\"{label.lower().replace('-', '_')}_store\"\n",
    "    persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(\"Loading embedding model (this may download weights on first run)...\")\n",
    "        embedder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "        # Embedding sanity check\n",
    "        sample = texts[:2]\n",
    "        emb_sample = embedder.embed_documents(sample)\n",
    "        if not emb_sample or len(emb_sample) != len(sample):\n",
    "            print(\"‚ùå Embedding sanity check failed.\")\n",
    "            return False\n",
    "\n",
    "        final_texts = [t for t in texts if t.strip()]\n",
    "        final_metas = [m for t, m in zip(texts, metadatas) if t.strip()]\n",
    "\n",
    "        if not final_texts:\n",
    "            print(\"‚ö†Ô∏è No non-empty texts to embed after filtering. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        vectordb = Chroma.from_texts(\n",
    "            texts=final_texts,\n",
    "            embedding=embedder,\n",
    "            metadatas=final_metas,\n",
    "            persist_directory=str(persist_dir),\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully persisted {label} store at: {persist_dir}\")\n",
    "        print(f\"   Total documents: {len(final_texts)}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while building {label} store: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# PIPELINE ENTRYPOINT\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"üöÄ Starting full ingestion + embedding pipeline\")\n",
    "    print(f\"üìÇ JSON Directory: {JSON_DIR}\")\n",
    "    print(f\"üíæ Vector Store Directory: {VECTORSTORE_DIR}\")\n",
    "    \n",
    "    if not JSON_DIR.exists():\n",
    "        print(f\"‚ùå JSON directory does not exist: {JSON_DIR}\")\n",
    "        return\n",
    "\n",
    "    raw_docs = load_all_jsons(JSON_DIR)\n",
    "    if not raw_docs:\n",
    "        print(\"‚ùå No JSON documents loaded. Please check JSON_DIR path and files.\")\n",
    "        return\n",
    "\n",
    "    texts, metadatas = normalize_and_extract(raw_docs)\n",
    "    \n",
    "    if not texts:\n",
    "        print(\"‚ùå No texts extracted from documents. Please check the JSON structure.\")\n",
    "        return\n",
    "\n",
    "    summary = {}\n",
    "    for label, model_name in EMBED_MODELS.items():\n",
    "        ok = build_and_persist_chroma(label, model_name, texts, metadatas)\n",
    "        summary[label] = ok\n",
    "\n",
    "    print(\"\\nüîö Pipeline finished. Summary:\")\n",
    "    for label, ok in summary.items():\n",
    "        status = '‚úÖ OK' if ok else '‚ùå Skipped/Failed'\n",
    "        print(f\" - {label}: {status}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af47cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain / Chroma imports\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "ROOT_DIR = Path().resolve().parent  # parent of notebooks/\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Import project paths\n",
    "from config.paths import DATA_DIR, VECTORSTORE1_DIR , VECTORSTORE2_DIR\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "JSON_DIR = DATA_DIR / \"processed\"\n",
    "VECTORSTORE_DIR = Path(VECTORSTORE2_DIR)\n",
    "VECTORSTORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBED_MODELS = {\n",
    "    \"MiniLM\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"BGE-Base\": \"BAAI/bge-base-en-v1.5\",\n",
    "    \"BGE-Large\": \"BAAI/bge-large-en-v1.5\",\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def list_json_files(json_dir: Path) -> List[Path]:\n",
    "    files = sorted(list(json_dir.glob(\"*.json\")))\n",
    "    print(f\"Found {len(files)} JSON files in {json_dir}\")\n",
    "    for f in files:\n",
    "        print(\" -\", f)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_json_file(path: Path) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def load_all_jsons(json_dir: Path) -> List[Any]:\n",
    "    files = list_json_files(json_dir)\n",
    "    all_docs = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            data = load_json_file(p)\n",
    "            all_docs.append({\"__source_file\": p.name, \"__raw\": data})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read {p.name}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "\n",
    "def normalize_and_extract(raw_documents: List[Dict[str, Any]]) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Extract texts and metadata from the JSON schema and chunk article texts\n",
    "    into smaller segments for better embedding performance.\n",
    "    \"\"\"\n",
    "\n",
    "    texts: List[str] = []\n",
    "    metadatas: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --- Initialize text splitter for intra-article chunking ---\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,        # ~350‚Äì450 words\n",
    "        chunk_overlap=100,     # keeps local context\n",
    "        separators=[\"\\n\\n\", \".\", \"?\", \"!\", \";\", \":\", \" \"]\n",
    "    )\n",
    "\n",
    "    for raw_doc in raw_documents:\n",
    "        source_name = raw_doc.get(\"__source_file\", \"unknown\")\n",
    "        data = raw_doc.get(\"__raw\")\n",
    "\n",
    "        if data is None or not isinstance(data, dict):\n",
    "            continue\n",
    "\n",
    "        document_title = data.get(\"document_title\", \"\")\n",
    "        preamble = data.get(\"preamble\", \"\")\n",
    "\n",
    "        # -------------------------\n",
    "        # PREAMBLE\n",
    "        # -------------------------\n",
    "        if preamble and preamble.strip():\n",
    "            metadata = {\n",
    "                \"source\": source_name,\n",
    "                \"document_title\": document_title,\n",
    "                \"section_type\": \"preamble\",\n",
    "                \"chapter\": \"Preamble\",\n",
    "                \"section\": \"Preamble\"\n",
    "            }\n",
    "            texts.append(preamble.strip())\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        # -------------------------\n",
    "        # PARTS ‚Üí ARTICLES ‚Üí CLAUSES\n",
    "        # -------------------------\n",
    "        parts = data.get(\"parts\", [])\n",
    "        if not isinstance(parts, list):\n",
    "            continue\n",
    "\n",
    "        for part in parts:\n",
    "            if not isinstance(part, dict):\n",
    "                continue\n",
    "\n",
    "            part_num = part.get(\"part_number\", \"\")\n",
    "            part_title = part.get(\"part_title\", \"\")\n",
    "            chapter_label = f\"Part {part_num}: {part_title}\".strip()\n",
    "\n",
    "            articles = part.get(\"articles\", [])\n",
    "            if not isinstance(articles, list):\n",
    "                continue\n",
    "\n",
    "            for article in articles:\n",
    "                if not isinstance(article, dict):\n",
    "                    continue\n",
    "\n",
    "                art_num = article.get(\"article_number\", \"\")\n",
    "                art_title = article.get(\"article_title\", \"\")\n",
    "                section_label = f\"Article {art_num}: {art_title}\".strip()\n",
    "\n",
    "                clauses = article.get(\"clauses\", [])\n",
    "                if not isinstance(clauses, list):\n",
    "                    continue\n",
    "\n",
    "                combined_text = \"\\n\\n\".join(\n",
    "                    [clause.strip() for clause in clauses if clause and clause.strip()]\n",
    "                )\n",
    "                if not combined_text:\n",
    "                    continue\n",
    "\n",
    "                # --- Chunk the combined article text ---\n",
    "                chunks = splitter.split_text(combined_text)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    metadata = {\n",
    "                        \"source\": source_name,\n",
    "                        \"document_title\": document_title,\n",
    "                        \"section_type\": \"article\",\n",
    "                        \"chapter\": chapter_label,\n",
    "                        \"section\": f\"{section_label} (chunk {i+1}/{len(chunks)})\",\n",
    "                        \"part_number\": part_num,\n",
    "                        \"article_number\": art_num,\n",
    "                        \"num_clauses\": len(clauses),\n",
    "                        \"chunk_index\": i + 1,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    }\n",
    "                    texts.append(chunk)\n",
    "                    metadatas.append(metadata)\n",
    "\n",
    "        # -------------------------\n",
    "        # SCHEDULES\n",
    "        # -------------------------\n",
    "        schedules = data.get(\"schedules\", [])\n",
    "        if isinstance(schedules, list):\n",
    "            for schedule in schedules:\n",
    "                if not isinstance(schedule, dict):\n",
    "                    continue\n",
    "\n",
    "                schedule_num = schedule.get(\"schedule_number\", \"\")\n",
    "                schedule_title = schedule.get(\"schedule_title\", \"\")\n",
    "\n",
    "                if schedule_title and schedule_title.strip():\n",
    "                    section_label = f\"Schedule {schedule_num}\".strip()\n",
    "                    metadata = {\n",
    "                        \"source\": source_name,\n",
    "                        \"document_title\": document_title,\n",
    "                        \"section_type\": \"schedule\",\n",
    "                        \"chapter\": \"Schedules\",\n",
    "                        \"section\": section_label,\n",
    "                        \"schedule_number\": schedule_num\n",
    "                    }\n",
    "                    texts.append(schedule_title.strip())\n",
    "                    metadatas.append(metadata)\n",
    "\n",
    "    # --- Log summary ---\n",
    "    print(f\"‚úÖ Normalized and extracted {len(texts)} chunks.\")\n",
    "    avg_len = sum(len(t) for t in texts) / max(len(texts), 1)\n",
    "    print(f\"üìè Average chunk length: {avg_len:.1f} characters\")\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be2ee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full ingestion + embedding pipeline\n",
      "üìÇ JSON Directory: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\n",
      "üíæ Vector Store Directory: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db2\n",
      "Found 10 JSON files in D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Bank and Financial Institution Act 2073.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Banking Offence and Punishment Act 2064.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Constitution of Nepal 2072.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\Electronic Commerce Act 2081.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\International Financial Transactions Act 2054.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Criminal Offences Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Income Tax Act 2058.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The Labour Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The National Civil (Code) Act 2074.json\n",
      " - D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\data\\processed\\The National Penal (Code) Act 2074.json\n",
      "‚úÖ Normalized and extracted 5482 chunks.\n",
      "üìè Average chunk length: 319.7 characters\n",
      "\n",
      "=== Building Chroma store for MiniLM (sentence-transformers/all-MiniLM-L6-v2) ===\n",
      "Loading embedding model (this may download weights on first run)...\n",
      "‚úÖ Successfully persisted MiniLM store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db2\\minilm_store\n",
      "   Total documents: 5482\n",
      "\n",
      "=== Building Chroma store for BGE-Base (BAAI/bge-base-en-v1.5) ===\n",
      "Loading embedding model (this may download weights on first run)...\n",
      "‚úÖ Successfully persisted BGE-Base store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db2\\bge_base_store\n",
      "   Total documents: 5482\n",
      "\n",
      "=== Building Chroma store for BGE-Large (BAAI/bge-large-en-v1.5) ===\n",
      "Loading embedding model (this may download weights on first run)...\n",
      "‚úÖ Successfully persisted BGE-Large store at: D:\\Fusemachine\\MyPocketLawyer-AI-Powered-Legal-Aid-Assistant-2\\chroma_db2\\bge_large_store\n",
      "   Total documents: 5482\n",
      "\n",
      "üîö Pipeline finished. Summary:\n",
      " - MiniLM: ‚úÖ OK\n",
      " - BGE-Base: ‚úÖ OK\n",
      " - BGE-Large: ‚úÖ OK\n"
     ]
    }
   ],
   "source": [
    "def build_and_persist_chroma(label: str, model_name: str, texts: List[str], metadatas: List[Dict[str, Any]]):\n",
    "    print(f\"\\n=== Building Chroma store for {label} ({model_name}) ===\")\n",
    "    if not texts:\n",
    "        print(f\"‚ö†Ô∏è No texts to embed for {label}. Skipping.\")\n",
    "        return False\n",
    "\n",
    "    persist_dir = VECTORSTORE_DIR / f\"{label.lower().replace('-', '_')}_store\"\n",
    "    persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(\"Loading embedding model (this may download weights on first run)...\")\n",
    "        embedder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "        # Embedding sanity check\n",
    "        sample = texts[:2]\n",
    "        emb_sample = embedder.embed_documents(sample)\n",
    "        if not emb_sample or len(emb_sample) != len(sample):\n",
    "            print(\"‚ùå Embedding sanity check failed.\")\n",
    "            return False\n",
    "\n",
    "        final_texts = [t for t in texts if t.strip()]\n",
    "        final_metas = [m for t, m in zip(texts, metadatas) if t.strip()]\n",
    "\n",
    "        if not final_texts:\n",
    "            print(\"‚ö†Ô∏è No non-empty texts to embed after filtering. Skipping.\")\n",
    "            return False\n",
    "\n",
    "        vectordb = Chroma.from_texts(\n",
    "            texts=final_texts,\n",
    "            embedding=embedder,\n",
    "            metadatas=final_metas,\n",
    "            persist_directory=str(persist_dir),\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully persisted {label} store at: {persist_dir}\")\n",
    "        print(f\"   Total documents: {len(final_texts)}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error while building {label} store: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# PIPELINE ENTRYPOINT\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"üöÄ Starting full ingestion + embedding pipeline\")\n",
    "    print(f\"üìÇ JSON Directory: {JSON_DIR}\")\n",
    "    print(f\"üíæ Vector Store Directory: {VECTORSTORE_DIR}\")\n",
    "    \n",
    "    if not JSON_DIR.exists():\n",
    "        print(f\"‚ùå JSON directory does not exist: {JSON_DIR}\")\n",
    "        return\n",
    "\n",
    "    raw_docs = load_all_jsons(JSON_DIR)\n",
    "    if not raw_docs:\n",
    "        print(\"‚ùå No JSON documents loaded. Please check JSON_DIR path and files.\")\n",
    "        return\n",
    "\n",
    "    texts, metadatas = normalize_and_extract(raw_docs)\n",
    "    \n",
    "    if not texts:\n",
    "        print(\"‚ùå No texts extracted from documents. Please check the JSON structure.\")\n",
    "        return\n",
    "\n",
    "    summary = {}\n",
    "    for label, model_name in EMBED_MODELS.items():\n",
    "        ok = build_and_persist_chroma(label, model_name, texts, metadatas)\n",
    "        summary[label] = ok\n",
    "\n",
    "    print(\"\\nüîö Pipeline finished. Summary:\")\n",
    "    for label, ok in summary.items():\n",
    "        status = '‚úÖ OK' if ok else '‚ùå Skipped/Failed'\n",
    "        print(f\" - {label}: {status}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeca45b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
