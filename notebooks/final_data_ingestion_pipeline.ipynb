{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb9893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagannath/my_poc_law/MyPocketLawyer-AI-Powered-Legal-Aid-Assistant/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import fitz\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# Updated imports for newer LangChain versions\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "\n",
    "# Make the project root visible for imports\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from config.paths import DATA_DIR , PROCESSED_DIR , VECTORSTORE_DIR\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795df6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# TEXT PROCESSING HELPERS\n",
    "# ==========================\n",
    "\n",
    "def load_pdf_text(pdf_path: Path) -> str:\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "def clean_legal_text(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'â€“', '-', text)\n",
    "    return text.strip()\n",
    "\n",
    "def segment_legal_sections(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split legal PDFs into chapters/parts and sections.\n",
    "    Handles:\n",
    "      - Chapter/Part with numbers or Roman numerals\n",
    "      - Multi-line chapter titles\n",
    "      - Sections kept together\n",
    "      - Optional clauses preserved but not split line-by-line\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    current_chapter = None\n",
    "    current_section = None\n",
    "    buffer = []\n",
    "\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "\n",
    "        # Detect Chapter or Part headers\n",
    "        chapter_match = re.match(r'^(Chapter|CHAPTER|Part|PART)[\\sâ€“-]*([IVXLC\\d]+)?\\s*(.*)', line)\n",
    "        if chapter_match:\n",
    "            # Flush previous buffer\n",
    "            if buffer:\n",
    "                sections.append({\n",
    "                    \"chapter\": current_chapter or \"Unspecified Chapter/Part\",\n",
    "                    \"section\": current_section,\n",
    "                    \"content\": \" \".join(buffer).strip()\n",
    "                })\n",
    "                buffer = []\n",
    "\n",
    "            # Start new chapter\n",
    "            current_chapter = f\"{chapter_match.group(1)} {chapter_match.group(2) or ''}\".strip()\n",
    "\n",
    "            # Check next line for possible title\n",
    "            if i + 1 < len(lines):\n",
    "                next_line = lines[i + 1]\n",
    "                # If it does NOT look like a section or clause, append as chapter title\n",
    "                if not re.match(r'^(\\d+[\\.\\)]\\s*)', next_line):\n",
    "                    current_chapter += f\" {next_line}\"\n",
    "                    i += 1  # Skip the title line\n",
    "\n",
    "            current_section = None\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Detect numbered sections (e.g., '1.' or '10)')\n",
    "        section_match = re.match(r'^(\\d+[\\.\\)]\\s*)(.*)', line)\n",
    "        if section_match:\n",
    "            # Flush previous buffer\n",
    "            if buffer:\n",
    "                sections.append({\n",
    "                    \"chapter\": current_chapter or \"Unspecified Chapter/Part\",\n",
    "                    \"section\": current_section,\n",
    "                    \"content\": \" \".join(buffer).strip()\n",
    "                })\n",
    "                buffer = []\n",
    "\n",
    "            # Start new section\n",
    "            current_section = line\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Optional: detect clauses (1), (2) but keep in same section\n",
    "        clause_match = re.findall(r'\\(\\d+\\)\\s*([^()]+)', line)\n",
    "        if clause_match:\n",
    "            for clause in clause_match:\n",
    "                buffer.append(clause.strip())\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Regular content line\n",
    "        buffer.append(line)\n",
    "        i += 1\n",
    "\n",
    "    # Flush remaining buffer\n",
    "    if buffer:\n",
    "        sections.append({\n",
    "            \"chapter\": current_chapter or \"Unspecified Chapter/Part\",\n",
    "            \"section\": current_section,\n",
    "            \"content\": \" \".join(buffer).strip()\n",
    "        })\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def save_json(data, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75719e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# MAIN PIPELINE\n",
    "# ==========================\n",
    "\n",
    "def process_legal_pdfs(data_dir: Path, processed_dir: Path):\n",
    "    all_docs = []\n",
    "    pdf_files = list(data_dir.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"âš ï¸ No PDF files found in {data_dir}\")\n",
    "        return []\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"ðŸ“„ Processing PDFs\"):\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        sections = segment_legal_sections(text)\n",
    "\n",
    "        for s in sections:\n",
    "            s[\"doc_id\"] = str(uuid.uuid4())\n",
    "            s[\"source_file\"] = pdf_path.name\n",
    "            all_docs.append(s)\n",
    "\n",
    "    save_json(all_docs, processed_dir / \"legal_docs.json\")\n",
    "    return all_docs\n",
    "\n",
    "def create_vector_store(data, persist_dir: Path):\n",
    "    texts = [item[\"content\"] for item in data]\n",
    "    metadatas = [\n",
    "        {\"chapter\": item[\"chapter\"], \"section\": item[\"section\"], \"source\": item[\"source_file\"]}\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    split_docs, split_meta = [], []\n",
    "\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = splitter.split_text(text)\n",
    "        for chunk in chunks:\n",
    "            split_docs.append(chunk)\n",
    "            split_meta.append(meta)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "    vectordb = Chroma.from_texts(\n",
    "        texts=split_docs,\n",
    "        embedding=embeddings,\n",
    "        metadatas=split_meta,\n",
    "        persist_directory=str(persist_dir)\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(f\"âœ… Vector store created at: {persist_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22b6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Legal Document Ingestion Pipeline...\n",
      "âš ï¸ No PDF files found in /home/jagannath/my_poc_law/MyPocketLawyer-AI-Powered-Legal-Aid-Assistant/data\n",
      "âš ï¸ No data processed.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ENTRY POINT\n",
    "# ==========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "    os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"ðŸš€ Starting Legal Document Ingestion Pipeline...\")\n",
    "\n",
    "    data = process_legal_pdfs(DATA_DIR, PROCESSED_DIR)\n",
    "    if data:\n",
    "        create_vector_store(data, VECTORSTORE_DIR)\n",
    "        print(\"ðŸŽ¯ Ingestion + Embedding completed successfully.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No data processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1d12d",
   "metadata": {},
   "source": [
    "## ðŸ§© Pipeline Summary\n",
    "\n",
    "### **Step 1 â€” JSON Generator**\n",
    "\n",
    "- Splits PDFs into **40-page chunks**.  \n",
    "- Sends each chunk to **Gemini** for structured JSON extraction (uses gemini document understanding).  \n",
    "- Merges partials and saves one JSON per document under `data/processed/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a6f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pathlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "CHUNK_SIZE = 40  # pages per sub-PDF\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from config.paths import DATA_DIR, PROCESSED_DIR\n",
    "\n",
    "DATA_DIR = DATA_DIR\n",
    "OUTPUT_DIR = PROCESSED_DIR\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an AI assistant for legal document structuring and retrieval.\n",
    "\n",
    "Task:\n",
    "Extract the full *hierarchical citation structure* from the attached legal PDF chunk and return it as valid JSON.\n",
    "\n",
    "The structure must reflect:\n",
    "- Parts (with their number + title)\n",
    "- Articles (number, title, clauses if any)\n",
    "- Clauses or sub-clauses under each Article\n",
    "- Schedules (if any)\n",
    "- The preamble (if found)\n",
    "\n",
    "JSON Schema:\n",
    "{\n",
    "  \"document_title\": str,\n",
    "  \"preamble\": str,\n",
    "  \"parts\": [\n",
    "    {\n",
    "      \"part_number\": str,\n",
    "      \"part_title\": str,\n",
    "      \"articles\": [\n",
    "        {\n",
    "          \"article_number\": str,\n",
    "          \"article_title\": str,\n",
    "          \"clauses\": [str]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"schedules\": [\n",
    "    {\n",
    "      \"schedule_number\": str,\n",
    "      \"schedule_title\": str\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Output ONLY JSON. No commentary, no markdown.\n",
    "- If a value doesnâ€™t exist, use \"\" or [].\n",
    "- Maintain exact hierarchical order.\n",
    "CRITICAL: Return valid JSON or \"{}\" â€” never text commentary.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def split_pdf(pdf_path, chunk_size=40):\n",
    "    \"\"\"Split a PDF into smaller temporary PDFs of `chunk_size` pages.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    chunks = []\n",
    "    total_pages = len(reader.pages)\n",
    "    for start in range(0, total_pages, chunk_size):\n",
    "        end = min(start + chunk_size, total_pages)\n",
    "        writer = PdfWriter()\n",
    "        for i in range(start, end):\n",
    "            writer.add_page(reader.pages[i])\n",
    "        chunk_path = pdf_path.parent / f\"__temp_{pdf_path.stem}_{start+1}-{end}.pdf\"\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks\n",
    "\n",
    "def clean_json(raw):\n",
    "    \"\"\"Extract valid JSON block from model output.\"\"\"\n",
    "    match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
    "    return match.group(0).strip() if match else raw.strip()\n",
    "\n",
    "def merge_jsons(json_list):\n",
    "    \"\"\"Merge multiple JSON structures safely (deduplicate by part/article).\"\"\"\n",
    "    if not json_list:\n",
    "        return {}\n",
    "\n",
    "    base = json_list[0].copy()\n",
    "    base_parts = {p[\"part_number\"]: p for p in base.get(\"parts\", [])}\n",
    "    base_schedules = {s[\"schedule_number\"]: s for s in base.get(\"schedules\", [])}\n",
    "\n",
    "    for js in json_list[1:]:\n",
    "        for part in js.get(\"parts\", []):\n",
    "            pnum = part[\"part_number\"]\n",
    "            if pnum in base_parts:\n",
    "                existing_articles = {a[\"article_number\"]: a for a in base_parts[pnum].get(\"articles\", [])}\n",
    "                for art in part.get(\"articles\", []):\n",
    "                    anum = art[\"article_number\"]\n",
    "                    if anum in existing_articles:\n",
    "                        # merge clauses (dedup)\n",
    "                        old_clauses = set(existing_articles[anum].get(\"clauses\", []))\n",
    "                        new_clauses = set(art.get(\"clauses\", []))\n",
    "                        existing_articles[anum][\"clauses\"] = list(old_clauses | new_clauses)\n",
    "                    else:\n",
    "                        base_parts[pnum][\"articles\"].append(art)\n",
    "            else:\n",
    "                base_parts[pnum] = part\n",
    "\n",
    "        for sch in js.get(\"schedules\", []):\n",
    "            snum = sch[\"schedule_number\"]\n",
    "            if snum not in base_schedules:\n",
    "                base_schedules[snum] = sch\n",
    "\n",
    "    base[\"parts\"] = list(base_parts.values())\n",
    "    base[\"schedules\"] = list(base_schedules.values())\n",
    "    return base\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "pdf_paths = list(DATA_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    print(f\"\\nðŸ“˜ Processing: {pdf_path.name}\")\n",
    "\n",
    "    # Step 1: Split into smaller chunks\n",
    "    chunk_paths = split_pdf(pdf_path, CHUNK_SIZE)\n",
    "    partial_jsons = []\n",
    "\n",
    "    # Step 2: Upload each chunk & extract JSON\n",
    "    for cpath in chunk_paths:\n",
    "        print(f\"  â†³ Analyzing pages {cpath.name.split('_')[-1].replace('.pdf','')}\")\n",
    "        uploaded = client.files.upload(file=cpath.open(\"rb\"), config={\"mime_type\": \"application/pdf\"})\n",
    "        resp = client.models.generate_content(model=\"gemini-2.5-flash\", contents=[uploaded, PROMPT])\n",
    "        raw = clean_json(resp.text)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "            if parsed:\n",
    "                partial_jsons.append(parsed)\n",
    "        except json.JSONDecodeError:\n",
    "            # Retry once with strict \"fix JSON\" instruction\n",
    "            retry_prompt = \"Rewrite ONLY this into valid JSON according to schema:\\n\" + raw\n",
    "            retry_resp = client.models.generate_content(model=\"gemini-2.5-flash\", contents=[retry_prompt])\n",
    "            raw_retry = clean_json(retry_resp.text)\n",
    "            try:\n",
    "                parsed_retry = json.loads(raw_retry)\n",
    "                partial_jsons.append(parsed_retry)\n",
    "            except:\n",
    "                print(f\"âš ï¸  Could not parse chunk {cpath.name}\")\n",
    "\n",
    "        # Cleanup temp chunk file\n",
    "        cpath.unlink(missing_ok=True)\n",
    "\n",
    "    # Step 3: Merge all chunk JSONs\n",
    "    merged = merge_jsons(partial_jsons)\n",
    "\n",
    "    # Step 4: Save single unified JSON per document\n",
    "    out_path = OUTPUT_DIR / f\"{pdf_path.stem}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Final JSON saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b89ee",
   "metadata": {},
   "source": [
    "### **Step 2 â€” Vector Store Builder**\n",
    "\n",
    "- Loads JSONs from `data/processed/`.  \n",
    "- Flattens hierarchical structure into **granular clauses**.  \n",
    "- Generates **embeddings** using Gemini.  \n",
    "- Saves vector database locally under `chroma_db/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c70b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Loading processed JSONs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 313.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Total 5037 text entries to embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding & storing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:58<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vectorstore created at: /home/jagannath/my_poc_law/MyPocketLawyer-AI-Powered-Legal-Aid-Assistant/chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from google import genai\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"âŒ Missing GEMINI_API_KEY in .env\")\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().parent  # when running inside a notebook\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from config.paths import DATA_DIR, PROCESSED_DIR, VECTORSTORE_DIR\n",
    "\n",
    "DATA_DIR = DATA_DIR\n",
    "OUTPUT_DIR = PROCESSED_DIR\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "COLLECTION_NAME = \"legal_docs\"\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_json_files(folder):\n",
    "    \"\"\"Load all JSON files from folder.\"\"\"\n",
    "    json_files = list(Path(folder).glob(\"*.json\"))\n",
    "    data = []\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as jf:\n",
    "                data.append((f.stem, json.load(jf)))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load {f.name}: {e}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_legal_json(doc_title, js):\n",
    "    \"\"\"\n",
    "    Flatten legal JSON:\n",
    "    Each clause becomes an entry with metadata for part, article, and source document.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "\n",
    "    # Preamble (optional)\n",
    "    if js.get(\"preamble\"):\n",
    "        entries.append({\n",
    "            \"text\": js[\"preamble\"].strip(),\n",
    "            \"metadata\": {\n",
    "                \"document_title\": doc_title,\n",
    "                \"section\": \"Preamble\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Parts and Articles\n",
    "    for part in js.get(\"parts\", []):\n",
    "        pnum = part.get(\"part_number\", \"\")\n",
    "        ptitle = part.get(\"part_title\", \"\")\n",
    "        for article in part.get(\"articles\", []):\n",
    "            anum = article.get(\"article_number\", \"\")\n",
    "            atitle = article.get(\"article_title\", \"\")\n",
    "            clauses = article.get(\"clauses\", [])\n",
    "\n",
    "            # Each clause stored separately for fine-grained retrieval\n",
    "            for idx, clause in enumerate(clauses, start=1):\n",
    "                entries.append({\n",
    "                    \"text\": clause.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"document_title\": doc_title,\n",
    "                        \"part_number\": pnum,\n",
    "                        \"part_title\": ptitle,\n",
    "                        \"article_number\": anum,\n",
    "                        \"article_title\": atitle,\n",
    "                        \"clause_index\": idx,\n",
    "                        \"section\": \"Clause\"\n",
    "                    }\n",
    "                })\n",
    "    return entries\n",
    "\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    \"\"\"Generate embeddings using Gemini.\"\"\"\n",
    "    response = client.models.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        contents=texts\n",
    "    )\n",
    "    return [e.values for e in response.embeddings]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "def build_chroma_store():\n",
    "    print(\"ðŸ“š Loading processed JSONs...\")\n",
    "    data = load_json_files(PROCESSED_DIR)\n",
    "\n",
    "    all_entries = []\n",
    "    for doc_name, js in tqdm(data, desc=\"Flattening documents\"):\n",
    "        all_entries.extend(flatten_legal_json(doc_name, js))\n",
    "\n",
    "    print(f\"ðŸ§© Total {len(all_entries)} text entries to embed\")\n",
    "\n",
    "    client_chroma = chromadb.PersistentClient(path=str(VECTORSTORE_DIR))\n",
    "\n",
    "    # Remove old collection if exists\n",
    "    if COLLECTION_NAME in [c.name for c in client_chroma.list_collections()]:\n",
    "        client_chroma.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "    collection = client_chroma.create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "    for i in tqdm(range(0, len(all_entries), BATCH_SIZE), desc=\"Embedding & storing\"):\n",
    "        batch = all_entries[i:i + BATCH_SIZE]\n",
    "        texts = [e[\"text\"] for e in batch]\n",
    "        metadatas = [e[\"metadata\"] for e in batch]\n",
    "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "\n",
    "        embeddings = get_embeddings(texts)\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… Vectorstore created at: {VECTORSTORE_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_chroma_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0051e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
